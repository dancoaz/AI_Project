{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code to collect tweets that mention Adidas:"
      ],
      "metadata": {
        "id": "fCqhah0AA6Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "consumer_key = 'your_consumer_key'\n",
        "consumer_secret = 'your_consumer_secret'\n",
        "access_token = 'your_access_token'\n",
        "access_token_secret = 'your_access_token_secret'\n",
        "\n",
        "# Authenticate with Twitter\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Create the API object\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Define the search query\n",
        "query = 'Adidas'\n",
        "\n",
        "# Collect tweets that match the search query\n",
        "tweets = []\n",
        "for tweet in tweepy.Cursor(api.search_tweets,\n",
        "                           q=query,\n",
        "                           tweet_mode='extended',\n",
        "                           lang='en').items(1000):\n",
        "    tweets.append(tweet.full_text)\n",
        "\n",
        "# Print the collected tweets\n",
        "print(tweets)\n"
      ],
      "metadata": {
        "id": "k4wVAd5TBGfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the tweets:"
      ],
      "metadata": {
        "id": "3tvECt61BHp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    tokens = word_tokenize(tweet.lower())\n",
        "    tokens = [stemmer.stem(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess the tweets\n",
        "preprocessed_tweets = [preprocess_tweet(tweet) for tweet in tweets]\n",
        "\n",
        "# Print the preprocessed tweets\n",
        "print(preprocessed_tweets)\n"
      ],
      "metadata": {
        "id": "F_1oH5txBS4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the tweets:"
      ],
      "metadata": {
        "id": "D4_jBVR2BVXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "labels = ['positive', 'negative', 'neutral'] # Replace with your own labels\n",
        "\n",
        "# Label the tweets\n",
        "# Replace the labels with your own labels\n",
        "y = ['positive', 'neutral', 'negative', 'neutral', 'positive', 'negative', ... ]\n",
        "\n",
        "# Split the preprocessed tweets into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_tweets, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "LnjvDQk4Bhx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using scikit-learn to train and evaluate our sentiment analyzer using Naive Bayes, Support Vector Machine, and Logistic Regression classifiers."
      ],
      "metadata": {
        "id": "ZB4l-JgGBlQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Vectorizing the tweets using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Training and evaluating Naive Bayes model\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vec, y_train)\n",
        "y_pred_nb = nb.predict(X_test_vec)\n",
        "print(\"Naive Bayes:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_nb))\n"
      ],
      "metadata": {
        "id": "vxq6dXxvBwWp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}